# üß† Annotation Automatique - Matching CV / Offre

## üìù Introduction

Ce module est d√©di√© √† l'**annotation automatique** d'un dataset √† partir d‚Äôun ensemble d'offres d‚Äôemploi et de CVs class√©s par domaine.  
L'objectif est de pr√©parer les donn√©es n√©cessaires pour entra√Æner un mod√®le de **matching intelligent** entre les offres et les profils de candidats.

---

## üéØ Objectif de l'annotation

Pour **chaque offre d‚Äôemploi**, un ensemble de **100 CVs** sera assign√©, r√©parti selon la similarit√© de leur domaine par rapport √† celui de l‚Äôoffre :

- 50 CVs du **m√™me domaine** que l‚Äôoffre
- 25 √† 30 CVs de **domaines similaires**
- 20 √† 25 CVs de **domaines √©loign√©s**

Cette organisation permet d‚Äôintroduire de la diversit√© et de simuler un sc√©nario r√©aliste pour la d√©tection automatique de bons et mauvais matchs.

---

## üîç D√©marche de s√©lection


### 1. üîé Collecte et identification du domaine des offres

Les offres sont collect√©es et stock√©es par domaine dans le dossier suivant :  
`scraping/offers/`

Exemple :  
`scraping/offers/data_science/offer1.txt` ‚Üí domaine = `data_science`

> üóíÔ∏è **Note** :  
> Si vous souhaitez consulter le processus complet de r√©cup√©ration et d'organisation des offres, veuillez vous r√©f√©rer au fichier [`job_scraper/docs.md`](job_scraper/docs.md).

---

### 2. üìÅ Organisation et classification des CVs

Les CVs sont √©galement organis√©s et stock√©s par domaine dans le dossier suivant :  
`scraping/cvs/{domaine}/cvX.txt`

Exemple :  
`scraping/cvs/data_science/cv1.txt` ‚Üí domaine = `data_science`

> üóíÔ∏è **Note** :  
> Si vous souhaitez consulter le processus complet de r√©cup√©ration et d'organisation des CVs, veuillez vous r√©f√©rer au fichier [`cv_scraper/docs.md`](cv_scraper/docs.md).

---


### 3. üéØ Constitution du jeu de N CVs par offre (100 cvs par exemple)
Pour chaque offre, les CVs sont s√©lectionn√©s et r√©partis selon les crit√®res suivants (les pourcentages et le nombre de CVs dans l'exemple sont √† titre indicatif, et la r√©partition exacte est g√©r√©e dans le notebook) :

| Type de domaine     | Pourcentage approx. | Nombre de CVs | Source                              |
|---------------------|----------------------|----------------|--------------------------------------|
| üü¢ M√™me domaine     | 50%                  | ~50            | M√™me dossier que l‚Äôoffre             |
| üü° Domaines proches | 25-30%               | ~25-30         | Dossiers manuellement d√©finis        |
| üî¥ Domaines lointains| 20-25%              | ~20-25         | Dossiers diff√©rents / √©loign√©s       |

Les CVs sont assign√©s **al√©atoirement** depuis ces dossiers pour garantir de la variabilit√©.

Par exemple, pour chaque offre, un total de **N CVs** sera s√©lectionn√©, r√©partis entre les trois types de domaines mentionn√©s ci-dessus. Le nombre exact de CVs par offre est sp√©cifi√© par le param√®tre `cvs_per_offer` dans le script.

La fonction de pr√©paration des CVs pour l'annotation est g√©r√©e par la fonction `prepare_to_annotate` qui est stock√©e dans `utils/utils.py`. 

Elle est appel√©e dans le notebook de la mani√®re suivante :

```python
# Appel de la fonction avec chemins relatifs corrects
prepare_to_annotate(
    offers_path="../scraping/offres",
    cvs_path="../scraping/cvs",
    domain_map=domain_map,
    output_path="ready_to_annotate.csv",
    cvs_per_offer=5
)
```
---
## üìù Processus d'annotation

Une fois les CVs s√©lectionn√©s pour chaque offre, le processus d'annotation commence. L'annotation consiste √† √©valuer la correspondance entre l'offre d'emploi et les CVs s√©lectionn√©s. Pour chaque CV, un score entre **0** et **1** est attribu√©, repr√©sentant la pertinence du CV par rapport √† l'offre d'emploi.

### üéØ Attribution du score / niveau

Ce score permettra de mesurer la correspondance s√©mantique entre l'offre et le CV, o√π un score plus proche de **1** indique une forte pertinence, et un score proche de **0** indique une faible pertinence.



---
## ü§ñ √âvaluation automatique via LLM

Pour automatiser l‚Äôannotation, nous avons test√© plusieurs mod√®les, dont **DeepSeek** accessible via la plateforme OpenRouter, **Ollama**, et **Groq**.

1. **DeepSeek via OpenRouter** : Ce mod√®le joue le r√¥le d‚Äôun expert RH virtuel et attribue un score de pertinence entre 0 et 1 √† chaque couple offre/CV, sur la base de leur compatibilit√© s√©mantique. Cependant, l'appel √† l'API de DeepSeek via OpenRouter s'est r√©v√©l√© relativement lent, ce qui a rendu son utilisation moins optimale pour un grand nombre d'annotations.

2. **Ollama** : Bien qu‚Äôil ait montr√© de bons r√©sultats en termes de pr√©cision, Ollama a √©galement montr√© des performances de traitement plus lentes, rendant l'annotation de grandes quantit√©s de donn√©es difficile.

3. **Groq** : Apr√®s avoir test√© plusieurs solutions, nous avons choisi **Groq** comme outil principal pour l'√©valuation des scores, car il a montr√© une meilleure r√©activit√© et efficacit√© pour traiter rapidement les offres et les CVs.

Le processus d'annotation est g√©r√© par la fonction `annotate_data` dans le fichier [annotate_data.ipynb](annotate_data.ipynb). Cette fonction :

- Effectue la lecture des fichiers CV et offres.
- G√©n√®re les prompts et envoie les requ√™tes au mod√®le (Groq dans notre cas).
- Enregistre les scores obtenus dans un fichier `annotated_data.csv`.

### Exemple de g√©n√©ration de prompt

Voici comment le prompt est g√©n√©r√© pour envoyer au mod√®le :

```python
def generate_prompt(offer_text, cv_text):
    return f"""
Tu es un expert en recrutement. √âvalue la pertinence de ce CV pour cette offre d‚Äôemploi.

### Offre :
{offer_text}

### CV :
{cv_text}

Donne uniquement un score de similarit√© entre 0 et 1 (float), sans aucune explication ni commentaire.
R√©ponds uniquement par le nombre.
"""

---

```



## üì¶ R√©sultat

les scores obtenus sont sauvegard√©s dans le fichier suivant :
[`data/labeled_data.csv`](../data/labeled_data.csv)

---